import React, { useState, useRef, useEffect, useCallback } from 'react';
import { Send, User, Bot, Mic, MicOff, Keyboard, Volume2 } from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import './AudioInput.css';

interface Message {
  id: string;
  content: string;
  role: 'user' | 'assistant';
  timestamp: Date;
  audioChunks?: AudioChunk[];
  isStreaming?: boolean;
}

interface AudioChunk {
  index: number;
  data: Blob;
  received: boolean;
}

interface WebSocketMessage {
  name?: string;
  payload?: any;
  index?: number;
  text?: string;
}

interface AudioMessage {
  type: string;
  index: number;
  sessionId: string;
  size: number;
}

// Audio input modes
type InputMode = 'typing' | 'listening';
type ListeningMode = 'passive' | 'active';

interface AudioInputState {
  isRecording: boolean;
  hasPermission: boolean;
  permissionError: string | null;
  isProcessing: boolean;
}

export const ChatInterface: React.FC = () => {
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      content: "Hello! I'm Xarvis, your AI assistant. I can help you with various tasks. I support real-time streaming responses with both text and audio output. How can I assist you today?",
      role: 'assistant',
      timestamp: new Date(),
    }
  ]);
  const [inputValue, setInputValue] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [connectionStatus, setConnectionStatus] = useState<'connecting' | 'connected' | 'disconnected'>('disconnected');

  // Audio input state
  const [inputMode, setInputMode] = useState<InputMode>('typing');
  const [listeningMode, setListeningMode] = useState<ListeningMode>('passive');
  const [audioInputState, setAudioInputState] = useState<AudioInputState>({
    isRecording: false,
    hasPermission: false,
    permissionError: null,
    isProcessing: false
  });

  // Refs
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLTextAreaElement>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const currentMessageRef = useRef<string>('');
  const currentMessageIdRef = useRef<string>('');
  const pendingAudioMetadataRef = useRef<AudioMessage | null>(null);

  // Audio recording refs
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef2 = useRef<AudioContext | null>(null);
  const audioStreamRef2 = useRef<MediaStream | null>(null);
  const audioWorkletRef = useRef<AudioWorkletNode | null>(null);
  const recordingStateRef = useRef<boolean>(false);

  // Audio queue and real-time playback
  const [audioQueue, setAudioQueue] = useState<AudioChunk[]>([]);
  const audioQueueRef = useRef<AudioChunk[]>([]);
  const audioContextRef = useRef<AudioContext | null>(null);
  const isPlayingRef = useRef<boolean>(false);

  // PCM audio format info (received from backend)
  const audioFormatRef = useRef<{
    format: string;
    sampleRate: number;
    channels: number;
    bitsPerSample: number;
    encoding: string;
  } | null>(null);

  // Continuous audio streaming state
  const audioStreamRef = useRef<{
    currentTime: number;
    nextStartTime: number;
    isStreaming: boolean;
  }>({
    currentTime: 0,
    nextStartTime: 0,
    isStreaming: false
  });

  // Track active audio sources for cleanup
  const activeAudioSourcesRef = useRef<AudioBufferSourceNode[]>([]);

  // Cleanup function to stop all active audio sources
  const stopAllAudio = () => {
    console.log(`🛑 Stopping ${activeAudioSourcesRef.current.length} active audio sources`);
    activeAudioSourcesRef.current.forEach(source => {
      try {
        source.stop();
      } catch (e) {
        // Source might already be stopped
      }
    });
    activeAudioSourcesRef.current = [];
    isPlayingRef.current = false;
    audioStreamRef.current.isStreaming = false;
  };  // Keep ref in sync with state
  useEffect(() => {
    audioQueueRef.current = audioQueue;
  }, [audioQueue]);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // Real-time audio player - automatically plays chunks from queue
  const playAudioQueue = async () => {
    // If already playing, ignore trigger
    if (isPlayingRef.current || audioStreamRef.current.isStreaming) {
      console.log(`🚫 Playback already active - ignoring trigger`);
      return;
    }

    // Safety cleanup - stop any lingering audio sources
    stopAllAudio();

    // Wait for a minimum buffer of chunks to reduce stuttering
    const minBufferChunks = 2;
    if (audioQueueRef.current.length < minBufferChunks) {
      console.log(`🔄 Waiting for buffer... (${audioQueueRef.current.length}/${minBufferChunks} chunks)`);
      return;
    }

    isPlayingRef.current = true;
    audioStreamRef.current.isStreaming = true;
    console.log(`🎵 Starting continuous audio playback. Queue length: ${audioQueueRef.current.length}`);

    try {
      // Initialize audio context if needed
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)();
      }

      // Resume audio context if suspended (browser policy)
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }

      // Initialize timing for continuous playback
      audioStreamRef.current.currentTime = audioContextRef.current.currentTime;
      audioStreamRef.current.nextStartTime = audioStreamRef.current.currentTime + 0.1; // Start slightly in future

      // Start continuous chunk scheduling
      scheduleNextChunk();

    } catch (error) {
      console.error('❌ Audio playback error:', error);
      isPlayingRef.current = false;
      audioStreamRef.current.isStreaming = false;
    }
  };

  // Schedule chunks for continuous playback without gaps
  const scheduleNextChunk = () => {
    if (!audioStreamRef.current.isStreaming || !audioContextRef.current) {
      return;
    }

    // Get current queue
    const currentQueue = audioQueueRef.current;

    // If queue is empty, stop playback
    if (currentQueue.length === 0) {
      console.log(`🎉 Continuous audio playback complete. Queue empty.`);
      isPlayingRef.current = false;
      audioStreamRef.current.isStreaming = false;
      return;
    }

    // Get the first chunk
    const currentChunk = currentQueue[0];
    console.log(`🔊 Scheduling chunk ${currentChunk.index} at time ${audioStreamRef.current.nextStartTime.toFixed(3)}`);

    // Schedule this chunk for continuous playback
    playChunkContinuous(currentChunk, audioStreamRef.current.nextStartTime)
      .then((duration) => {
        console.log(`✅ Chunk ${currentChunk.index} scheduled successfully (${duration.toFixed(3)}s)`);

        // Update next start time for seamless continuity
        audioStreamRef.current.nextStartTime += duration;

        // Remove played chunk from queue
        setAudioQueue(prev => {
          const newQueue = prev.slice(1);
          console.log(`🗑️ Removed chunk ${currentChunk.index}. Remaining: ${newQueue.length} chunks`);
          return newQueue;
        });

        // Schedule next chunk immediately (no delay!)
        setTimeout(() => scheduleNextChunk(), 10);
      })
      .catch(error => {
        console.error(`❌ Failed to schedule chunk ${currentChunk.index}:`, error);

        // Remove failed chunk and continue
        setAudioQueue(prev => {
          const newQueue = prev.slice(1);
          console.log(`🗑️ Removed failed chunk ${currentChunk.index}. Remaining: ${newQueue.length} chunks`);
          return newQueue;
        });

        // Continue with next chunk
        setTimeout(() => scheduleNextChunk(), 10);
      });
  };

  // Play PCM audio chunk with precise timing for continuous playback
  const playChunkContinuous = async (chunk: AudioChunk, startTime: number): Promise<number> => {
    if (!audioContextRef.current) {
      throw new Error('Audio context not available');
    }

    if (!audioFormatRef.current) {
      throw new Error('Audio format not received yet');
    }

    try {
      const arrayBuffer = await chunk.data.arrayBuffer();

      // Check if we have valid audio data
      if (arrayBuffer.byteLength === 0) {
        throw new Error('Empty audio buffer');
      }

      const format = audioFormatRef.current;
      const samplesCount = arrayBuffer.byteLength / (format.bitsPerSample / 8);

      // Create AudioBuffer for PCM data
      const audioBuffer = audioContextRef.current.createBuffer(
        format.channels,
        samplesCount,
        format.sampleRate
      );

      // Convert raw PCM bytes to float32 samples
      const channelData = audioBuffer.getChannelData(0);

      if (format.bitsPerSample === 16 && format.encoding === 's16le') {
        // 16-bit signed little-endian PCM
        const samples = new Int16Array(arrayBuffer);
        for (let i = 0; i < samples.length; i++) {
          channelData[i] = samples[i] / 32768; // Convert to [-1, 1] range
        }
      } else {
        throw new Error(`Unsupported PCM format: ${format.bitsPerSample}-bit ${format.encoding}`);
      }

      // Schedule the audio buffer to play at precise time
      const source = audioContextRef.current.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContextRef.current.destination);

      // Track this source for cleanup
      activeAudioSourcesRef.current.push(source);

      // Remove from tracking when it ends
      source.onended = () => {
        const index = activeAudioSourcesRef.current.indexOf(source);
        if (index > -1) {
          activeAudioSourcesRef.current.splice(index, 1);
        }
      };

      // Start at the precise scheduled time for seamless playback
      source.start(startTime);

      // Return the duration of this chunk for timing next chunk
      const duration = audioBuffer.duration;
      console.log(`⏰ Chunk ${chunk.index} duration: ${duration.toFixed(3)}s, scheduled at: ${startTime.toFixed(3)}s`);

      return duration;

    } catch (error: any) {
      console.error(`🔍 PCM Chunk ${chunk.index} scheduling error:`, error);
      console.error(`🔍 Chunk size: ${chunk.data.size} bytes`);
      throw error;
    }
  };  // Trigger audio playback intelligently - only when we have buffer AND not already playing
  useEffect(() => {
    // Only trigger if:
    // 1. Not already playing/streaming
    // 2. Have enough chunks in buffer 
    // 3. Actually have chunks to play
    if (!isPlayingRef.current &&
      !audioStreamRef.current.isStreaming &&
      audioQueue.length >= 2) {
      console.log(`🚀 Triggering playback: ${audioQueue.length} chunks in buffer`);
      playAudioQueue();
    }
  }, [audioQueue]);

  // Request microphone permissions
  const requestAudioPermission = useCallback(async () => {
    // Set processing state
    setAudioInputState(prev => ({
      ...prev,
      isProcessing: true,
      permissionError: null
    }));

    // Check if mediaDevices API is available
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      const errorMsg = 'Media devices not supported in this browser or context';
      console.error('❌ ' + errorMsg);
      setAudioInputState(prev => ({
        ...prev,
        hasPermission: false,
        permissionError: errorMsg,
        isProcessing: false
      }));
      return false;
    }

    // Check if we're in a secure context (HTTPS or localhost)
    if (!window.isSecureContext && window.location.hostname !== 'localhost') {
      const errorMsg = 'Microphone access requires HTTPS or localhost';
      console.error('❌ ' + errorMsg);
      setAudioInputState(prev => ({
        ...prev,
        hasPermission: false,
        permissionError: errorMsg,
        isProcessing: false
      }));
      return false;
    }

    try {
      console.log('🎤 Requesting microphone permission...');

      // First check if permission is already granted
      if (navigator.permissions) {
        try {
          const permissionStatus = await navigator.permissions.query({ name: 'microphone' as PermissionName });
          console.log('Current permission state:', permissionStatus.state);

          if (permissionStatus.state === 'denied') {
            throw new Error('Microphone permission was previously denied. Please enable it in browser settings.');
          }
        } catch (permError) {
          // Permission API might not be fully supported, continue with getUserMedia
          console.log('Permission API not fully supported, proceeding with getUserMedia');
        }
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      // Test successful - we have permission
      setAudioInputState(prev => ({
        ...prev,
        hasPermission: true,
        permissionError: null,
        isProcessing: false
      }));

      // Stop the test stream
      stream.getTracks().forEach(track => track.stop());
      console.log('✅ Microphone permission granted');

      return true;
    } catch (error: any) {
      console.error('❌ Microphone permission denied:', error);

      let errorMessage = 'Microphone access denied';

      if (error.name === 'NotAllowedError') {
        errorMessage = 'Microphone permission denied by user. Please allow microphone access and try again.';
      } else if (error.name === 'NotFoundError') {
        errorMessage = 'No microphone found. Please check your audio devices.';
      } else if (error.name === 'NotReadableError') {
        errorMessage = 'Microphone is already in use by another application.';
      } else if (error.name === 'OverconstrainedError') {
        errorMessage = 'Microphone does not support the requested audio format.';
      } else if (error.name === 'SecurityError') {
        errorMessage = 'Microphone access blocked by security policy. Try using HTTPS or localhost.';
      } else if (error.message) {
        errorMessage = error.message;
      }

      setAudioInputState(prev => ({
        ...prev,
        hasPermission: false,
        permissionError: errorMessage,
        isProcessing: false
      }));
      return false;
    }
  }, []);

  // Start audio recording with real-time PCM streaming
  const startAudioRecording = useCallback(async () => {
    if (recordingStateRef.current) return;

    console.log('🎤 Starting audio recording...');

    try {
      // Get microphone stream (let browser choose optimal sample rate)
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      });

      audioStreamRef2.current = stream;

      // Create audio context for PCM processing (use default sample rate)
      audioContextRef2.current = new (window.AudioContext || (window as any).webkitAudioContext)();

      const source = audioContextRef2.current.createMediaStreamSource(stream);

      // Create a script processor for real-time PCM extraction (larger buffer for efficiency)
      const processor = audioContextRef2.current.createScriptProcessor(16384, 1, 1); // 16KB buffer instead of 4KB

      let frameCount = 0;
      let audioBuffer: Int16Array[] = [];
      let lastSendTime = Date.now();
      const SEND_INTERVAL_MS = 200; // Send audio every 200ms instead of every frame
      const MAX_BUFFER_SIZE = 5; // Maximum number of frames to buffer

      processor.onaudioprocess = (e) => {
        if (!recordingStateRef.current) {
          return;
        }

        const inputBuffer = e.inputBuffer;
        const inputData = inputBuffer.getChannelData(0);

        // Convert float32 to int16 PCM
        const pcmData = new Int16Array(inputData.length);
        for (let i = 0; i < inputData.length; i++) {
          pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
        }

        // Add to buffer
        audioBuffer.push(pcmData);
        frameCount++;

        // Send buffered audio every SEND_INTERVAL_MS or when buffer is full
        const now = Date.now();
        if (now - lastSendTime >= SEND_INTERVAL_MS || audioBuffer.length >= MAX_BUFFER_SIZE) {
          if (audioBuffer.length > 0) {
            // Combine all buffered frames into one larger chunk
            const totalSamples = audioBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
            const combinedPcm = new Int16Array(totalSamples);
            let offset = 0;

            for (const chunk of audioBuffer) {
              combinedPcm.set(chunk, offset);
              offset += chunk.length;
            }

            // Calculate stats for logging
            const maxAmplitude = Math.max(...Array.from(combinedPcm).map(sample => Math.abs(sample / 32768)));
            const actualSampleRate = audioContextRef2.current?.sampleRate || 44100;

            // Send combined chunk
            sendAudioFrame(combinedPcm.buffer, actualSampleRate, 1);

            // Clear buffer
            audioBuffer = [];
            lastSendTime = now;
          }
        }
      };

      source.connect(processor);
      processor.connect(audioContextRef2.current.destination);

      recordingStateRef.current = true;
      setAudioInputState(prev => ({ ...prev, isRecording: true }));

      console.log('✅ Audio recording started successfully!');
      console.log(`🎤 Audio context state: ${audioContextRef2.current.state}`);
      console.log(`🎤 Audio context sample rate: ${audioContextRef2.current.sampleRate}Hz`);
      console.log(`🎤 Stream active: ${stream.active}`);
      console.log(`🎤 Stream tracks: ${stream.getTracks().length}`);
      console.log(`🎤 Stream settings:`, stream.getTracks()[0]?.getSettings());

    } catch (error: any) {
      console.error('❌ Failed to start audio recording:', error);
      setAudioInputState(prev => ({
        ...prev,
        permissionError: error.message || 'Failed to start recording'
      }));
    }
  }, []);

  // Stop audio recording
  const stopAudioRecording = useCallback(() => {
    console.log('🛑 Stopping audio recording...');

    recordingStateRef.current = false;

    if (audioStreamRef2.current) {
      console.log(`🛑 Stopping ${audioStreamRef2.current.getTracks().length} audio tracks`);
      audioStreamRef2.current.getTracks().forEach(track => track.stop());
      audioStreamRef2.current = null;
    }

    if (audioContextRef2.current) {
      console.log('🛑 Closing audio context');
      audioContextRef2.current.close();
      audioContextRef2.current = null;
    }

    setAudioInputState(prev => ({ ...prev, isRecording: false }));
    console.log('✅ Audio recording stopped completely');
  }, []);

  // Audio flow control state
  const audioSendQueueRef = useRef<ArrayBuffer[]>([]);
  const isSendingAudioRef = useRef(false);
  const audioSendRateLimitRef = useRef({ count: 0, resetTime: Date.now() });

  // Send PCM audio frame to backend with flow control
  const sendAudioFrame = useCallback((pcmBuffer: ArrayBuffer, sampleRate: number, channels: number) => {
    if (wsRef.current?.readyState !== WebSocket.OPEN) {
      console.warn(`📡 WebSocket not connected (state: ${wsRef.current?.readyState}), dropping audio frame`);
      return;
    }

    // Rate limiting: max 10 audio frames per second
    const now = Date.now();
    if (now - audioSendRateLimitRef.current.resetTime > 1000) {
      audioSendRateLimitRef.current = { count: 0, resetTime: now };
    }

    if (audioSendRateLimitRef.current.count >= 10) {
      console.warn('📡 Audio rate limit exceeded, dropping frame');
      return;
    }

    // Queue size limit: drop frames if queue is too full
    if (audioSendQueueRef.current.length > 3) {
      console.warn('📡 Audio send queue full, dropping oldest frame');
      audioSendQueueRef.current.shift(); // Remove oldest frame
    }

    // Create binary message: 4 bytes sample rate + 2 bytes channels + 2 bytes padding + PCM data
    const header = new ArrayBuffer(8);
    const headerView = new DataView(header);

    headerView.setUint32(0, sampleRate, true); // Little endian
    headerView.setUint16(4, channels, true);   // Little endian
    headerView.setUint16(6, 0, true);          // Reserved padding

    // Combine header and PCM data
    const combinedBuffer = new Uint8Array(header.byteLength + pcmBuffer.byteLength);
    combinedBuffer.set(new Uint8Array(header), 0);
    combinedBuffer.set(new Uint8Array(pcmBuffer), header.byteLength);

    // Add to queue and process
    audioSendQueueRef.current.push(combinedBuffer.buffer);
    audioSendRateLimitRef.current.count++;

    // Process queue
    processAudioSendQueue();

    console.log(`📡 QUEUED AUDIO: ${pcmBuffer.byteLength} bytes PCM, ${sampleRate}Hz, ${channels}ch, queue size: ${audioSendQueueRef.current.length}`);
  }, []);

  // Process audio send queue with throttling
  const processAudioSendQueue = useCallback(() => {
    if (isSendingAudioRef.current || audioSendQueueRef.current.length === 0) {
      return;
    }

    isSendingAudioRef.current = true;

    const processNext = () => {
      if (audioSendQueueRef.current.length === 0) {
        isSendingAudioRef.current = false;
        return;
      }

      const buffer = audioSendQueueRef.current.shift();
      if (buffer && wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(buffer);
      }

      // Small delay to prevent overwhelming the WebSocket
      setTimeout(processNext, 50); // 50ms delay between sends
    };

    processNext();
  }, []);

  const connectWebSocket = useCallback(() => {
    if (wsRef.current?.readyState === WebSocket.OPEN) return;

    setConnectionStatus('connecting');

    // Use appropriate WebSocket URL based on environment
    const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsHost = window.location.host;
    const wsUrl = `${wsProtocol}//${wsHost}/ws`;

    wsRef.current = new WebSocket(wsUrl);

    wsRef.current.onopen = () => {
      console.log('🔗 WebSocket connected successfully!');
      console.log(`🔗 WebSocket URL: ${wsUrl}`);
      console.log(`🔗 WebSocket ready state: ${wsRef.current?.readyState}`);
      setConnectionStatus('connected');
    };

    wsRef.current.onclose = () => {
      console.log('WebSocket disconnected');
      setConnectionStatus('disconnected');
      setIsLoading(false);
      // Attempt to reconnect after a delay
      setTimeout(connectWebSocket, 3000);
    };

    wsRef.current.onerror = (error) => {
      console.error('WebSocket error:', error);
      setConnectionStatus('disconnected');
      setIsLoading(false);
    };

    wsRef.current.onmessage = (event) => {
      if (typeof event.data === 'string') {
        // Handle JSON messages (text deltas, events, and audio metadata)
        console.log("recv JSON:", event)
        try {
          const msg = JSON.parse(event.data);

          if (msg.type === 'audio_meta') {
            // Store audio metadata for next binary message
            pendingAudioMetadataRef.current = msg as AudioMessage;
          } else if (typeof msg.index === 'number' && msg.text) {
            // Text delta message
            handleTextDelta(msg.index, msg.text);
          } else if (msg.name && msg.payload) {
            // Event message
            handleEvent(msg.name, msg.payload);
          }
        } catch (error) {
          console.error('Error parsing WebSocket message:', error);
        }
      } else {
        // Handle binary messages (audio frames) - store only, no playback
        if (pendingAudioMetadataRef.current) {
          // Use metadata from previous message
          handleAudioFrame(pendingAudioMetadataRef.current, event.data);
          pendingAudioMetadataRef.current = null;
        } else {
          // Fallback to legacy handling
          handleLegacyAudioFrame(event.data);
        }
      }
    };
  }, []);

  const handleTextDelta = (index: number, text: string) => {
    console.log(`handleTextDelta: index=${index}, text="${text}", currentMessageId=${currentMessageIdRef.current}`);

    if (index === 1 || !currentMessageIdRef.current) {
      // Start of new message (index starts at 1, not 0) - stop any existing audio
      console.log('🔄 New message starting - stopping previous audio');
      stopAllAudio();

      const messageId = Date.now().toString();
      currentMessageIdRef.current = messageId;
      currentMessageRef.current = text;

      // Reset audio queue for new message
      setAudioQueue([]);

      const newMessage: Message = {
        id: messageId,
        content: text,
        role: 'assistant',
        timestamp: new Date(),
        isStreaming: true,
        audioChunks: [],
      };

      console.log('Creating new message:', newMessage);
      setMessages(prev => {
        const newMessages = [...prev, newMessage];
        console.log('Updated messages:', newMessages);
        return newMessages;
      });
    } else {
      // Continue streaming message
      currentMessageRef.current += text;
      console.log('Updating message content to:', currentMessageRef.current);

      setMessages(prev => prev.map(msg =>
        msg.id === currentMessageIdRef.current
          ? { ...msg, content: currentMessageRef.current }
          : msg
      ));
    }
  };

  // Handle audio frames - simple PCM chunk queuing
  const handleAudioFrame = async (audioMeta: AudioMessage, audioData: Blob) => {
    const chunkSizeKB = (audioData.size / 1024).toFixed(2);
    console.log(`📦 PCM Audio chunk ${audioMeta.index}: ${chunkSizeKB} KB`);

    // Verify data integrity
    if (audioData.size !== audioMeta.size) {
      console.warn(`⚠️ Size mismatch chunk ${audioMeta.index}: expected ${audioMeta.size}, got ${audioData.size}`);
    }

    // Skip empty chunks
    if (audioData.size === 0) {
      console.warn(`⚠️ Chunk ${audioMeta.index} is empty, skipping`);
      return;
    }

    try {
      // Create audio chunk - PCM data is directly playable
      const audioChunk: AudioChunk = {
        index: audioMeta.index,
        data: audioData, // Raw PCM data
        received: true
      };

      // Add to queue in order
      setAudioQueue(prev => {
        const newQueue = [...prev, audioChunk].sort((a, b) => a.index - b.index);
        console.log(`🎵 PCM Queue updated: ${newQueue.length} chunks [${newQueue.map(c => c.index).join(', ')}]`);
        return newQueue;
      });

      console.log(`✅ Added PCM chunk ${audioMeta.index} to queue`);

      // Trigger playback if not already playing and we have enough buffer
      if (!isPlayingRef.current && !audioStreamRef.current.isStreaming) {
        setTimeout(() => playAudioQueue(), 50);
      }

    } catch (error) {
      console.error(`Error processing PCM audio chunk ${audioMeta.index}:`, error);
    }
  };

  // Handle legacy audio frames
  const handleLegacyAudioFrame = async (audioData: Blob) => {
    console.log('handleLegacyAudioFrame called - converting to indexed audio');

    try {
      const audioChunk: AudioChunk = {
        index: 1, // Legacy audio is treated as single chunk
        data: audioData, // Use raw data directly
        received: true
      };

      // Add to queue and log length
      setAudioQueue([audioChunk]);
      console.log(`🎵 Queue updated (legacy): 1 chunk [1]`);

    } catch (error) {
      console.error('Error processing legacy audio frame:', error);
    }
  };

  const handleEvent = async (eventName: string, payload: any) => {
    console.log('Received event:', eventName, payload);

    if (eventName === 'audio_format') {
      // Store PCM format information from backend
      audioFormatRef.current = payload;
      console.log('🎵 Audio format received:', payload);
    } else if (eventName === 'message_complete') {
      // Mark current message as complete (no buffering needed for PCM)
      setMessages(prev => prev.map(msg =>
        msg.id === currentMessageIdRef.current
          ? { ...msg, isStreaming: false }
          : msg
      ));

      console.log(`Message complete. Audio queue length: ${audioQueue.length}`);
      setIsLoading(false);
      currentMessageRef.current = '';
    } else if (eventName === 'audio_complete') {
      console.log(`Audio complete event received. Total chunks: ${payload?.totalChunks}. Current queue length: ${audioQueue.length}`);
    } else if (eventName === 'listening_mode_change') {
      // Backend can trigger active/passive mode changes
      const newMode = payload?.data;
      console.log("listening mode change", newMode, payload)
      if (newMode === 'active' || newMode === 'passive') {
        console.log(`🎧 Backend requested listening mode change: ${listeningMode} -> ${newMode}`);
        setListeningMode(newMode as ListeningMode);
      }
    }
  };

  // Handle input mode changes
  const handleInputModeChange = useCallback(async (mode: InputMode) => {
    if (mode === inputMode) return;

    console.log(`🔄 Switching input mode: ${inputMode} -> ${mode}`);
    console.log(`🔍 Current audio state:`, audioInputState);

    if (mode === 'listening') {
      // Request permission first
      console.log(`🎤 Requesting audio permission...`);
      const hasPermission = await requestAudioPermission();
      console.log(`🎤 Permission result:`, hasPermission);

      if (!hasPermission) {
        console.error('Cannot switch to listening mode - no microphone permission');
        return;
      }

      // Start recording in passive mode
      console.log(`🎤 Switching to listening mode and starting recording...`);
      setInputMode('listening');
      setListeningMode('passive');
      await startAudioRecording();

    } else {
      // Switch to typing mode
      console.log(`⌨️ Switching to typing mode and stopping recording...`);
      stopAudioRecording();
      setInputMode('typing');
    }
  }, [inputMode, requestAudioPermission, startAudioRecording, stopAudioRecording, audioInputState]);



  // Initialize and cleanup
  useEffect(() => {
    connectWebSocket();

    // Don't request audio permission on mount to avoid early permission prompts
    // Permission will be requested when user switches to listening mode

    return () => {
      wsRef.current?.close();

      // Stop audio recording
      stopAudioRecording();

      // Stop all audio playback and clean up
      stopAllAudio();

      // Clean up audio contexts
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
      if (audioContextRef2.current) {
        audioContextRef2.current.close();
      }
    };
  }, [connectWebSocket, stopAudioRecording]);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!inputValue.trim() || isLoading || connectionStatus !== 'connected') return;

    const userMessage: Message = {
      id: Date.now().toString(),
      content: inputValue,
      role: 'user',
      timestamp: new Date(),
    };

    setMessages(prev => [...prev, userMessage]);
    setInputValue('');
    setIsLoading(true);

    if (wsRef.current?.readyState === WebSocket.OPEN) {
      wsRef.current.send(inputValue);
    } else {
      setIsLoading(false);
      console.error('WebSocket is not connected');
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSubmit(e as any);
    }
  };

  const adjustTextareaHeight = () => {
    if (inputRef.current) {
      inputRef.current.style.height = 'auto';
      inputRef.current.style.height = `${Math.min(inputRef.current.scrollHeight, 120)}px`;
    }
  };

  useEffect(() => {
    adjustTextareaHeight();
  }, [inputValue]);

  const getConnectionStatusColor = () => {
    switch (connectionStatus) {
      case 'connected': return '#4ade80';
      case 'connecting': return '#fbbf24';
      case 'disconnected': return '#ef4444';
      default: return '#6b7280';
    }
  };

  return (
    <div className="chat-container">
      <div className="chat-header">
        <h2 className="chat-title">Chat with Xarvis</h2>
        <p className="chat-subtitle">
          Real-time AI assistant with streaming text and audio
        </p>
        <div className="connection-status">
          <div
            className="status-dot"
            style={{ backgroundColor: getConnectionStatusColor() }}
          ></div>
          <span style={{ textTransform: 'capitalize' }}>{connectionStatus}</span>
        </div>
      </div>

      <div className="messages-container">
        {messages.map((message) => (
          <div key={message.id} className={`message ${message.role}`}>
            <div className={`message-avatar ${message.role}`}>
              {message.role === 'user' ? <User size={16} /> : <Bot size={16} />}
            </div>
            <div className="message-content">
              <div className="message-text">
                {message.role === 'assistant' ? (
                  <ReactMarkdown>{message.content}</ReactMarkdown>
                ) : (
                  <p>{message.content}</p>
                )}
                {message.isStreaming && (
                  <span className="streaming-cursor">|</span>
                )}
              </div>
              {message.role === 'assistant' && message.audioChunks && message.audioChunks.length > 0 && (
                <div className="message-audio">
                  <div style={{ display: 'flex', gap: '0.5rem', alignItems: 'center', marginBottom: '0.25rem' }}>
                    <span style={{ fontSize: '0.75rem', color: '#888' }}>
                      🎵 Audio playing in real-time ({message.audioChunks.length} chunks processed)
                      {message.isStreaming && ' - [STREAMING]'}
                    </span>
                  </div>
                </div>
              )}
            </div>
          </div>
        ))}

        {isLoading && (
          <div className="loading-indicator">
            <Bot size={16} />
            <span>Xarvis is responding...</span>
            <div className="loading-dots">
              <span></span>
              <span></span>
              <span></span>
            </div>
          </div>
        )}

        <div ref={messagesEndRef} />
      </div>

      <div className="input-container">
        {/* Input Mode Toggle */}
        <div className="input-mode-toggle">
          <button
            type="button"
            onClick={() => handleInputModeChange('typing')}
            className={`mode-button ${inputMode === 'typing' ? 'active' : ''}`}
            title="Switch to typing mode"
          >
            <Keyboard size={16} />
            <span>Type</span>
          </button>
          <button
            type="button"
            onClick={() => handleInputModeChange('listening')}
            className={`mode-button ${inputMode === 'listening' ? 'active' : ''}`}
            title="Switch to listening mode"
            disabled={!!audioInputState.permissionError || audioInputState.isProcessing}
          >
            <Mic size={16} />
            <span>{audioInputState.isProcessing ? 'Requesting...' : 'Listen'}</span>
          </button>
        </div>

        {/* Text Input Form (when in typing mode) */}
        {inputMode === 'typing' && (
          <form onSubmit={handleSubmit} className="input-form">
            <textarea
              ref={inputRef}
              value={inputValue}
              onChange={(e) => setInputValue(e.target.value)}
              onKeyPress={handleKeyPress}
              onInput={adjustTextareaHeight}
              placeholder={
                connectionStatus === 'connected'
                  ? "Type your message to Xarvis..."
                  : `Cannot send messages - ${connectionStatus}`
              }
              className="input-field"
              disabled={isLoading || connectionStatus !== 'connected'}
              rows={1}
            />
            <button
              type="submit"
              disabled={!inputValue.trim() || isLoading || connectionStatus !== 'connected'}
              className="send-button"
            >
              <Send size={18} />
            </button>
          </form>
        )}

        {/* Audio Input Controls (when in listening mode) */}
        {inputMode === 'listening' && (
          <div className="audio-input-controls">
            {audioInputState.permissionError ? (
              <div className="audio-error">
                <MicOff size={20} />
                <div className="error-content">
                  <span className="error-title">Microphone Access Issue</span>
                  <span className="error-message">{audioInputState.permissionError}</span>
                  <div className="error-actions">
                    <button
                      type="button"
                      onClick={requestAudioPermission}
                      className="retry-button"
                    >
                      Try Again
                    </button>
                    <button
                      type="button"
                      onClick={() => handleInputModeChange('typing')}
                      className="fallback-button"
                    >
                      Use Typing Instead
                    </button>
                  </div>
                </div>
              </div>
            ) : (
              <div className="audio-controls">
                <div className="listening-status">
                  <div className={`listening-indicator ${listeningMode}`}>
                    {audioInputState.isRecording ? (
                      <Mic size={20} className="recording" />
                    ) : (
                      <MicOff size={20} />
                    )}
                  </div>
                  <div className="listening-info">
                    <span className="listening-mode">
                      {listeningMode === 'passive' ? 'Passive Listening' : 'Active Listening'}
                    </span>
                    <span className="listening-status-text">
                      {audioInputState.isRecording
                        ? '🎤 Listening for voice input...'
                        : 'Microphone ready'
                      }
                    </span>
                  </div>
                </div>

                {/* Listening Mode Display */}
                <div className="listening-mode-controls">
                  <div className={`listening-mode-indicator ${listeningMode}`}>
                    {listeningMode === 'passive' ? (
                      <>
                        <Volume2 size={16} />
                        <span>Passive Listening</span>
                      </>
                    ) : (
                      <>
                        <Mic size={16} />
                        <span>Active Listening</span>
                      </>
                    )}
                  </div>

                  {/* Push-to-talk button */}
                  <button
                    type="button"
                    className="push-to-talk"
                    onMouseDown={() => setListeningMode('active')}
                    onMouseUp={() => setListeningMode('passive')}
                    onMouseLeave={() => setListeningMode('passive')}
                    title="Hold to activate push-to-talk"
                  >
                    Hold to Talk
                  </button>
                </div>
              </div>
            )}
          </div>
        )}

        {/* Audio Permission Status */}
        {!audioInputState.hasPermission && !audioInputState.permissionError && (
          <div className="audio-permission-notice">
            <span>🎤 Microphone permission needed for voice input</span>
            {!window.isSecureContext && window.location.hostname !== 'localhost' && (
              <span style={{ fontSize: '12px', marginTop: '4px', color: '#dc3545' }}>
                ⚠️ Voice input requires HTTPS or localhost
              </span>
            )}
          </div>
        )}
      </div>
    </div>
  );
};